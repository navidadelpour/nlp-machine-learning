{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"nn-homeword2.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"Gh5Bp4ZtXG8P","colab_type":"code","colab":{}},"source":["import os\n","import sys\n","import re\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","from google.colab import drive\n","from sklearn.model_selection import train_test_split\n","from keras.preprocessing.sequence import pad_sequences\n","from gensim.models import Word2Vec\n","from keras.layers import Add, LSTM, Bidirectional, Input, Embedding, Dense, Concatenate, TimeDistributed, Activation, Permute, Flatten, Multiply\n","from keras.models import Sequential,Model\n","import numpy as np\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mJo8Iz-mXPaU","colab_type":"code","outputId":"cfd988b0-e759-4679-90d6-d9e81d152b7b","executionInfo":{"status":"ok","timestamp":1563540479678,"user_tz":-270,"elapsed":3040,"user":{"displayName":"Navid Adelpour","photoUrl":"https://lh3.googleusercontent.com/-mAcQMj8zeb8/AAAAAAAAAAI/AAAAAAAAAwA/8VuZFUj2GbU/s64/photo.jpg","userId":"00725423770723278141"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["drive.mount('/content/gdrive/')\n","\n","root_path = '/content/gdrive/My Drive/Colab Notebooks/nn-homework2'\n","sys.path.append(root_path)"],"execution_count":43,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eQ86FhenXSIx","colab_type":"code","colab":{}},"source":["class DataFetcher(object):\n","\n","    def __init__(self, data_preproccessor, max_data_count):\n","        self.data_preproccessor = data_preproccessor\n","        self.max_data_count = max_data_count\n","        return super().__init__()\n","\n","    def fetch(self, filename, categories):\n","        text = self._read_file(filename)\n","        text_array = self._text_to_array(text)\n","        data = self._split_text_array_to_categories(text_array, categories)\n","        return data\n","\n","    def _read_file(self, file_name):\n","        with open(file_name,'r', encoding = \"ISO-8859-1\") as readin:\n","            text = readin.read()\n","        return text\n","\n","    def _text_to_array(self, text):\n","        return list(filter(lambda x: x != \"\", text.split(\"\\n\")))\n","\n","    def _split_text_array_to_categories(self, text_array, categories):\n","        data = {}\n","        for category in categories:\n","            data[category] = []\n","        data_count = 0\n","        for i in range(len(text_array)):\n","            if (i + 1) % 8 == 0 and i != 0 and data_count < self.max_data_count:\n","                for j in range(len(categories)):\n","                    valid_clean_data = self.data_preproccessor.clean(text_array[i - j], categories[j])\n","                    data[categories[j]].append(valid_clean_data)\n","                data_count += 1\n","        return data\n","      "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0GphRU1hYEzQ","colab_type":"code","colab":{}},"source":["class DataPreproccessor(object):\n","\n","    contraction_mapping = {\n","        \"ain't\":\"is not\",\"aren't\":\"are not\",\"can't\":\"cannot\",\"'cause\":\"because\",\"could've\":\"could have\",\"couldn't\":\"could not\",\"didn't\":\"did not\",\"doesn't\":\"does not\",\"don't\":\"do not\",\"hadn't\":\"had not\",\"hasn't\":\"has not\",\"haven't\":\"have not\",\"he'd\":\"he would\",\"he'll\":\"he will\",\"he's\":\"he is\",\"how'd\":\"how did\",\"how'd'y\":\"how do you\",\"how'll\":\"how will\",\"how's\":\"how is\", \"I'd\":\"I would\",\"I'd've\":\"I would have\",\"I'll\":\"I will\",\"I'll've\":\"I will have\",\"I'm\":\"I am\",\"I've\":\"I have\",\"i'd\":\"i would\", \"i'd've\":\"i would have\",\"i'll\":\"i will\", \"i'll've\":\"i will have\",\"i'm\":\"i am\",\"i've\":\"i have\",\"isn't\":\"is not\",\"it'd\":\"it would\", \"it'd've\":\"it would have\",\"it'll\":\"it will\",\"it'll've\":\"it will have\",\"it's\":\"it is\",\"let's\":\"let us\",\"ma'am\":\"madam\", \"mayn't\":\"may not\",\"might've\":\"might have\",\"mightn't\":\"might not\",\"mightn't've\":\"might not have\",\"must've\":\"must have\", \"mustn't\":\"must not\",\"mustn't've\":\"must not have\",\"needn't\":\"need not\",\"needn't've\":\"need not have\",\"o'clock\":\"of the clock\", \"oughtn't\":\"ought not\",\"oughtn't've\":\"ought not have\",\"shan't\":\"shall not\",\"sha'n't\":\"shall not\",\"shan't've\":\"shall not have\", \"she'd\":\"she would\",\"she'd've\":\"she would have\",\"she'll\":\"she will\",\"she'll've\":\"she will have\",\"she's\":\"she is\", \"should've\":\"should have\",\"shouldn't\":\"should not\",\"shouldn't've\":\"should not have\",\"so've\":\"so have\",\"so's\":\"so as\", \"this's\":\"this is\",\"that'd\":\"that would\",\"that'd've\":\"that would have\",\"that's\":\"that is\",\"there'd\":\"there would\", \"there'd've\":\"there would have\",\"there's\":\"there is\",\"here's\":\"here is\",\"they'd\":\"they would\",\"they'd've\":\"they would have\", \"they'll\":\"they will\",\"they'll've\":\"they will have\",\"they're\":\"they are\",\"they've\":\"they have\",\"to've\":\"to have\", \"wasn't\":\"was not\",\"we'd\":\"we would\",\"we'd've\":\"we would have\",\"we'll\":\"we will\",\"we'll've\":\"we will have\",\"we're\":\"we are\", \"we've\":\"we have\",\"weren't\":\"were not\",\"what'll\":\"what will\",\"what'll've\":\"what will have\",\"what're\":\"what are\", \"what's\":\"what is\",\"what've\":\"what have\",\"when's\":\"when is\",\"when've\":\"when have\",\"where'd\":\"where did\",\"where's\":\"where is\", \"where've\":\"where have\",\"who'll\":\"who will\",\"who'll've\":\"who will have\",\"who's\":\"who is\",\"who've\":\"who have\", \"why's\":\"why is\",\"why've\":\"why have\",\"will've\":\"will have\",\"won't\":\"will not\",\"won't've\":\"will not have\", \"would've\":\"would have\",\"wouldn't\":\"would not\",\"wouldn't've\":\"would not have\",\"y'all\":\"you all\", \"y'all'd\":\"you all would\",\"y'all'd've\":\"you all would have\",\"y'all're\":\"you all are\",\"y'all've\":\"you all have\", \"you'd\":\"you would\",\"you'd've\":\"you would have\",\"you'll\":\"you will\",\"you'll've\":\"you will have\", \"you're\":\"you are\",\"you've\":\"you have\"\n","    }\n","\n","\n","    def __init__(self):\n","        nltk.download('stopwords')\n","        self.stop_words = set(stopwords.words('english'))\n","        self.stemmer = PorterStemmer()\n","        self.clean_functions = [\n","            self._clean_from_html,\n","            self._clean_from_contraction_mapping,\n","            self._clean_from_punctuation,\n","            self._clean_numbers,\n","            self._clean_less_characters,\n","            self._remove_stop_words,\n","            self._stem_words,\n","        ]\n","        return super().__init__()\n","\n","    def clean(self, data, category):\n","        data = self._remove_category_label(data, category).lower()\n","        for func in self.clean_functions:\n","            data = func(data)\n","        return data\n","    \n","    def _remove_category_label(self, data, category):\n","        return data.replace(\"review/%s: \"%category, \"\")\n","\n","    def _clean_from_html(self, data):\n","        return re.sub(re.compile('<.*?>'),'',data)\n","\n","    def _clean_from_contraction_mapping(self, data):\n","        return self._do_on_word(data, map, lambda word: self.contraction_mapping.get(word, word))\n","\n","    def _clean_from_punctuation(self, data):\n","        return re.sub(r'[?|!|$|#|\\'|\"|:|,|(|)|.|\\|/]',r'',data)\n","\n","    def _do_on_word(self, data, action, func):\n","        return \" \".join(list(action(func, data.split(\" \"))))\n","\n","    def _clean_numbers(self, data):\n","        return self._do_on_word(data, filter, lambda word: word.isalpha())\n","\n","    def _clean_less_characters(self, data):\n","        return self._do_on_word(data, filter, lambda word: len(word) > 2)\n","\n","    def _remove_stop_words(self, data):\n","        return self._do_on_word(data, filter, lambda word: word not in self.stop_words)\n","\n","    def _stem_words(self, data):\n","        return self._do_on_word(data, map, lambda word: self.stemmer.stem(word))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bpY6QlntbtHL","colab_type":"code","colab":{}},"source":["class WordEmbeder(object):\n","    \n","    def __init__(self, dataset, epochs):\n","        self.epochs = epochs\n","        nltk.download('punkt')\n","        self._make_model(dataset)\n","        \n","        return super().__init__()\n","\n","    def _get_all_words(self, dataset):\n","        all_sentences = []\n","        for category_collection in list(dataset.values()):\n","            all_sentences += category_collection\n","        all_words = [sent.split(\" \") for sent in all_sentences]\n","        return all_words\n","\n","    def _make_model(self, dataset):\n","        corpus = self._get_all_words(dataset)\n","        model = Word2Vec(corpus, min_count=1)\n","        model.train(corpus, total_examples=len(corpus),epochs=self.epochs)\n","        self.model = model\n","\n","    def replace_word_with_vector(self, dataset):\n","        dataset_vec = {}\n","        for category in list(dataset.keys()):\n","            dataset_vec[category] = []\n","            for sentence in dataset[category]:\n","                dataset_vec[category].append([])\n","                for word in sentence.split(\" \"):\n","                    dataset_vec[category][-1].append(self.model.wv.word_vec(word))\n","        return dataset_vec\n","    \n","    def replace_vector_with_word(self, sentence):\n","      answer = []\n","      for word in sentence:\n","        if(word.all() != 0):\n","          new_word = self.model.wv.most_similar([word], topn=1)[0][0]\n","          answer.append(new_word)\n","      return ' '.join(answer)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cGd03IqiZkTp","colab_type":"code","colab":{}},"source":["class DataPreparer(object):\n","\n","    def __init__(self, max_words, train_test_ratio):\n","        self.train_test_ratio = train_test_ratio\n","        self.max_words = max_words\n","        return super().__init__()\n","\n","    def _pad_sequence(self, dataset):\n","        for category in list(dataset.keys()):\n","            dataset[category] = pad_sequences(dataset[category], maxlen=self.max_words[category], dtype=np.float32)\n","        return dataset\n","\n","    def _split_data(self, dataset):\n","        return train_test_split(\n","            dataset[categories[0]],\n","            dataset[categories[1]],\n","            test_size=self.train_test_ratio)\n","\n","    def prepare(self, dataset):\n","      return self._split_data(self._pad_sequence(dataset))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wZZzhrYEKlD7","colab_type":"code","colab":{}},"source":["class LstmModel(object):\n","  \n","  lstm_size = 128\n","  dropout = .2\n","  batch_size = 32\n","  \n","  def __init__(self, has_attention=False):\n","    self.has_attention = has_attention\n","    return super().__init__()\n","  \n","  \n","  def make_model(\n","      self,\n","      has_summary,\n","      word_len, \n","      max_words_summary,\n","      x_train,\n","      y_train,\n","      x_test,\n","      y_test,\n","      to_predict,\n","      epochs\n","  ):\n","    encoder_inputs = Input(x_train[0].shape)\n","    \n","    encoder_lstm = Bidirectional(LSTM(\n","        units = self.lstm_size,\n","        dropout = self.dropout,\n","        activation=\"tanh\",\n","        return_state=True,\n","    ))\n","\n","    encoder_outputs, h1, c1, h2, c2 = encoder_lstm(encoder_inputs)\n","\n","    decoder_inputs = Input(shape=(None, y_train[0].shape[1]))\n","\n","    decoder_lstm = LSTM(\n","        units = self.lstm_size,\n","        dropout = self.dropout,\n","        activation=\"softmax\",\n","        return_sequences=True,\n","        return_state=True,\n","    )\n","\n","    decoder_initial_state = [Add()([h1, h2]), Add()([c1, c2])]\n","\n","    decoder_outputs, h1, c1 = decoder_lstm(\n","        decoder_inputs,\n","        decoder_initial_state\n","    )\n","    \n","    if(self.has_attention):\n","      attention = TimeDistributed(Dense(1, activation='tanh'))\n","      decoder_outputs = attention(decoder_outputs)\n","\n","    dense = Dense(\n","        y_train[0].shape[1],\n","        activation='linear',\n","    )\n","\n","    outputs = dense(decoder_outputs)\n","\n","    model = Model([encoder_inputs, decoder_inputs], outputs)\n","\n","    if(has_summary):\n","      model.summary()\n","      \n","    model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n","    \n","    model.fit(\n","        x=[x_train,y_train],\n","        y=y_train,\n","        batch_size=self.batch_size,\n","        epochs=epochs,\n","        validation_data=([x_test,y_test], y_test)\n","    )\n","    \n","    scores = model.evaluate([x_test,y_test],y_test, verbose=0)\n","  \n","    encoder_model = Model(\n","        encoder_inputs,\n","        decoder_initial_state\n","    )\n","\n","    decoder_state = [\n","        Input((self.lstm_size,)),\n","        Input((self.lstm_size,)),\n","    ]\n","\n","    decoder_out, h1, c1 = decoder_lstm(\n","        decoder_inputs,\n","        initial_state = decoder_state\n","    )\n","\n","    if(self.has_attention):\n","      attention = TimeDistributed(Dense(1, activation='tanh'))\n","      decoder_out = attention(decoder_out)\n","\n","    \n","    output = dense(decoder_out)\n","\n","    decoder_model = Model(\n","        [decoder_inputs] + decoder_state, [output, h1, c1]\n","    )\n","\n","\n","    h1, c1 = encoder_model.predict(x_train)\n","    out = np.zeros((len(x_train),1, word_len))\n","    output = []\n","    for i in range(max_words_summary):\n","        out, h1, c1 = decoder_model.predict([out, h1, c1])\n","        output.append(out)\n","\n","    prediction = np.concatenate(output, axis=1)\n","    return prediction"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yqMTFZLpS7gY","colab_type":"code","colab":{}},"source":["# parameters\n","filename = 'finefoods.txt'\n","categories = [\"text\", \"summary\"]\n","max_words = {'text':200, \"summary\": 10}\n","train_test_ratio = .2\n","word2vec_train_epochs = 10\n","model_train_epochs = 5\n","word_len = 100\n","max_data_count = 10000"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"A3Xsl_qVS9wp","colab_type":"code","outputId":"778d76c1-50e3-4de8-89c5-59969985aba6","executionInfo":{"status":"ok","timestamp":1563541060076,"user_tz":-270,"elapsed":15456,"user":{"displayName":"Navid Adelpour","photoUrl":"https://lh3.googleusercontent.com/-mAcQMj8zeb8/AAAAAAAAAAI/AAAAAAAAAwA/8VuZFUj2GbU/s64/photo.jpg","userId":"00725423770723278141"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["# read & clean\n","data_preproccessor = DataPreproccessor()\n","data_fetcher = DataFetcher(data_preproccessor, max_data_count)\n","dataset = data_fetcher.fetch(os.path.join(root_path, filename), categories)"],"execution_count":58,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"prMLUFweTBfa","colab_type":"code","outputId":"cefbb289-3015-48df-db6b-a2b8f8152d15","executionInfo":{"status":"ok","timestamp":1563541068483,"user_tz":-270,"elapsed":20479,"user":{"displayName":"Navid Adelpour","photoUrl":"https://lh3.googleusercontent.com/-mAcQMj8zeb8/AAAAAAAAAAI/AAAAAAAAAwA/8VuZFUj2GbU/s64/photo.jpg","userId":"00725423770723278141"}},"colab":{"base_uri":"https://localhost:8080/","height":124}},"source":["# word2vec embedding\n","word_embedding_model = WordEmbeder(dataset, word2vec_train_epochs)\n","dataset_vecorized = word_embedding_model.replace_word_with_vector(dataset)\n","vocabulary = word_embedding_model.model.wv.syn0\n"],"execution_count":59,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"},{"output_type":"stream","text":["W0719 12:57:42.030803 139621052536704 base_any2vec.py:1182] Effective 'alpha' higher than previous training cycles\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n","  This is separate from the ipykernel package so we can avoid doing imports until\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"2sIoueGNTDMz","colab_type":"code","colab":{}},"source":["# pad & split\n","data_preparer = DataPreparer(max_words,train_test_ratio)\n","x_train, x_test, y_train, y_test = data_preparer.prepare(dataset_vecorized)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AoGtgGGLRDbX","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"zshaXEu_TFbd","colab_type":"code","outputId":"0dbcc714-d2d5-43e6-e9df-6b4f4adf10fe","executionInfo":{"status":"ok","timestamp":1563541963156,"user_tz":-270,"elapsed":896633,"user":{"displayName":"Navid Adelpour","photoUrl":"https://lh3.googleusercontent.com/-mAcQMj8zeb8/AAAAAAAAAAI/AAAAAAAAAwA/8VuZFUj2GbU/s64/photo.jpg","userId":"00725423770723278141"}},"colab":{"base_uri":"https://localhost:8080/","height":677}},"source":["# lstm model\n","lstm_model = LstmModel(has_attention=True)\n","prediction = lstm_model.make_model(\n","    has_summary = True,\n","    word_len = word_len, \n","    max_words_summary = max_words['summary'],\n","    x_train = x_train,\n","    y_train = y_train,\n","    x_test = x_test,\n","    y_test = y_test,\n","    to_predict = y_test,\n","    epochs = model_train_epochs\n",")\n"],"execution_count":61,"outputs":[{"output_type":"stream","text":["__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_7 (InputLayer)            (None, 200, 100)     0                                            \n","__________________________________________________________________________________________________\n","bidirectional_4 (Bidirectional) [(None, 256), (None, 234496      input_7[0][0]                    \n","__________________________________________________________________________________________________\n","input_8 (InputLayer)            (None, None, 100)    0                                            \n","__________________________________________________________________________________________________\n","add_7 (Add)                     (None, 128)          0           bidirectional_4[0][1]            \n","                                                                 bidirectional_4[0][3]            \n","__________________________________________________________________________________________________\n","add_8 (Add)                     (None, 128)          0           bidirectional_4[0][2]            \n","                                                                 bidirectional_4[0][4]            \n","__________________________________________________________________________________________________\n","lstm_8 (LSTM)                   [(None, None, 128),  117248      input_8[0][0]                    \n","                                                                 add_7[0][0]                      \n","                                                                 add_8[0][0]                      \n","__________________________________________________________________________________________________\n","time_distributed_4 (TimeDistrib (None, None, 1)      129         lstm_8[0][0]                     \n","__________________________________________________________________________________________________\n","dense_8 (Dense)                 (None, None, 100)    200         time_distributed_4[0][0]         \n","==================================================================================================\n","Total params: 352,073\n","Trainable params: 352,073\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","Train on 8000 samples, validate on 2000 samples\n","Epoch 1/5\n","8000/8000 [==============================] - 163s 20ms/step - loss: 0.2165 - acc: 0.0089 - val_loss: 0.1986 - val_acc: 0.0075\n","Epoch 2/5\n","8000/8000 [==============================] - 156s 20ms/step - loss: 0.2008 - acc: 0.0149 - val_loss: 0.1911 - val_acc: 0.0191\n","Epoch 3/5\n","8000/8000 [==============================] - 159s 20ms/step - loss: 0.1967 - acc: 0.0245 - val_loss: 0.1886 - val_acc: 0.0273\n","Epoch 4/5\n","8000/8000 [==============================] - 162s 20ms/step - loss: 0.1951 - acc: 0.0161 - val_loss: 0.1874 - val_acc: 0.0134\n","Epoch 5/5\n","8000/8000 [==============================] - 164s 21ms/step - loss: 0.1942 - acc: 0.0129 - val_loss: 0.1869 - val_acc: 0.0135\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"S1nnhd5UXfUd","colab_type":"code","outputId":"882b6d68-98d5-40c3-d46c-66038cab5a9e","executionInfo":{"status":"ok","timestamp":1563541963203,"user_tz":-270,"elapsed":893118,"user":{"displayName":"Navid Adelpour","photoUrl":"https://lh3.googleusercontent.com/-mAcQMj8zeb8/AAAAAAAAAAI/AAAAAAAAAwA/8VuZFUj2GbU/s64/photo.jpg","userId":"00725423770723278141"}},"colab":{"base_uri":"https://localhost:8080/","height":124}},"source":["# check the prediction\n","print(\"actual text: \" + word_embedding_model.replace_vector_with_word(x_test[0]))\n","print(\"actual summary: \" + word_embedding_model.replace_vector_with_word(y_test[0]))\n","print(\"predicted summary: \" + word_embedding_model.replace_vector_with_word(prediction[0]))\n"],"execution_count":62,"outputs":[{"output_type":"stream","text":["actual text: anyon see these made avoid treat china problem late see countri origin\n","actual summary: china\n","predicted summary: alot alot alot alot alot alot alot alot alot alot\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n","  if np.issubdtype(vec.dtype, np.int):\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"qPD5BUeymRbV","colab_type":"text"},"source":[""]}]}